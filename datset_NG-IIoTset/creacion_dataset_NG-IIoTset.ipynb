{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "239514d3-ffbb-42de-9a6f-87583bfa0bc1",
   "metadata": {},
   "source": [
    "# Creación del dataset NG-IIoTset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2619845b-5a6d-45b3-971c-29d183909825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ipaddress\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc12972-ba09-4661-b0f0-6780d288b9df",
   "metadata": {},
   "source": [
    "## Definición de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf9cf0-8dcd-4bfe-9ad0-619df475b77d",
   "metadata": {},
   "source": [
    "### create_ng_iiotset()\n",
    "Funcion que crea NG-IIoTset: Una Nueva Generación de conjunto de datos de seguridad para IIoT\n",
    "basado en Edge-IIoTset pero con una distribución personalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fe61206-9c3a-4a74-8977-d260681a6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ng_iiotset():\n",
    "\n",
    "    # 1. Definición de la distribución personalizada (manteniendo ±5% de variación respecto al original)\n",
    "\n",
    "    target_distribution = {\n",
    "        'normal': 10662000,               # Ligeramente reducido respecto al original\n",
    "        'backdoor': 30000,                # Aumentado para tener más representación\n",
    "        'ddos_http': 240000,              # Ligeramente aumentado\n",
    "        'ddos_icmp': 2850000,             # Ligeramente reducido\n",
    "        'ddos_tcp_syn': 2100000,          # Ligeramente aumentado\n",
    "        'ddos_udp': 3100000,              # Ligeramente reducido\n",
    "        'os_fingerprint': 5000,           # Aumentado para mejor representación\n",
    "        'mitm_arp_dns': 6000,             # Aumentado para mejor representación\n",
    "        'password': 1000000,              # Prácticamente igual\n",
    "        'port_scan': 45000,               # Duplicado para dar énfasis\n",
    "        'ransomware': 25000,              # Aumentado por relevancia actual\n",
    "        'sql_injection': 60000,           # Ligeramente aumentado\n",
    "        'upload': 50000,                  # Aumentado\n",
    "        'vuln_scan': 150000,              # Prácticamente igual\n",
    "        'xss': 25000                      # Aumentado por relevancia\n",
    "    }\n",
    "    \n",
    "    # 2. Procesar todos los archivos Zeek en una única pasada\n",
    "    all_data = {}\n",
    "    \n",
    "    # Definir las columnas a extraer para cada tipo de log\n",
    "    log_columns = {\n",
    "        'conn': ['uid', 'ts', 'id.orig_h', 'id.resp_h', 'id.resp_p', 'proto', 'service',\n",
    "                 'conn_state', 'duration', 'orig_bytes', 'resp_bytes', 'orig_pkts',\n",
    "                 'resp_pkts', 'ip_proto'],\n",
    "        'dns': ['uid', 'query', 'answers', 'qtype_name', 'rcode', 'rcode_name'],\n",
    "        'mqtt_connect': ['uid', 'connect_status', 'client_id'],\n",
    "        'mqtt_publish': ['uid', 'topic', 'payload'],\n",
    "        'modbus': ['uid', 'func', 'pdu_type', 'exception'],\n",
    "        'http': ['uid', 'method', 'uri', 'user_agent', 'host',\n",
    "                 'request_body_len', 'response_body_len', 'status_code'],\n",
    "        'files': ['uid', 'fuid', 'source', 'mime_type', 'filename',\n",
    "                 'seen_bytes', 'total_bytes', 'md5', 'sha1', 'sha256'],\n",
    "        'weird': ['uid', 'name'],\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Procesar primero los archivos conn.log ya que contienen la mayor parte de información\n",
    "    print(\"Procesando archivos conn.log...\")\n",
    "    base_df = process_conn_logs(normal_logs_dir, attack_logs_dir, log_columns['conn'])\n",
    "    \n",
    "    if base_df.empty:\n",
    "        print(\"Error: No se pudieron procesar los archivos conn.log\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Base DataFrame creada con {len(base_df)} registros\")\n",
    "    \n",
    "    # Procesar los demás logs y unirlos al DataFrame base\n",
    "    for log_type in ['dns', 'mqtt_connect', 'mqtt_publish', 'modbus', 'http', 'files', 'weird']:\n",
    "        print(f\"Procesando archivos {log_type}.log...\")\n",
    "        log_df = process_log_type(log_type, normal_logs_dir, attack_logs_dir, log_columns[log_type])\n",
    "        \n",
    "        if not log_df.empty:\n",
    "            print(f\"  - Encontrados {len(log_df)} registros, fusionando...\")\n",
    "            # Fusionar por uid\n",
    "            base_df = pd.merge(base_df, log_df, on='uid', how='left')\n",
    "    \n",
    "    # 3. Aplicar muestreo estratificado personalizado\n",
    "    print(\"Aplicando muestreo estratificado personalizado...\")\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for attack_type, target_count in target_distribution.items():\n",
    "        subset = base_df[base_df['typeAttack'] == attack_type]\n",
    "        print(f\"  - {attack_type}: {len(subset)} registros originales, objetivo {target_count}\")\n",
    "        \n",
    "        if len(subset) == 0:\n",
    "            print(f\"    ¡Advertencia! No hay registros del tipo {attack_type}\")\n",
    "            continue\n",
    "        \n",
    "        # Estrategia de muestreo\n",
    "        if len(subset) > target_count:\n",
    "            # Si hay más registros de los necesarios, realizar muestreo\n",
    "            sampled = subset.sample(n=target_count, random_state=42)\n",
    "            print(f\"    Reduciendo mediante muestreo a {len(sampled)} registros\")\n",
    "        else:\n",
    "            # Si hay menos, duplicar registros hasta alcanzar el objetivo\n",
    "            factor = target_count // len(subset)\n",
    "            remainder = target_count % len(subset)\n",
    "            \n",
    "            if factor > 1:\n",
    "                # Duplicar registros\n",
    "                duplicated = pd.concat([subset] * factor)\n",
    "                # Añadir los registros restantes\n",
    "                remaining = subset.sample(n=remainder, random_state=42)\n",
    "                sampled = pd.concat([duplicated, remaining])\n",
    "                print(f\"    Aumentando mediante duplicación a {len(sampled)} registros\")\n",
    "            else:\n",
    "                # Suficientes para el objetivo o cercano\n",
    "                sampled = subset\n",
    "                print(f\"    Manteniendo los {len(sampled)} registros originales\")\n",
    "        \n",
    "        final_df = pd.concat([final_df, sampled])\n",
    "    \n",
    "    # 4. Limpieza final\n",
    "    print(\"Realizando limpieza final...\")\n",
    "    # Eliminar columnas duplicadas que puedan haberse generado en las uniones\n",
    "    \n",
    "    # Rellenar valores nulos\n",
    "    for col in final_df.columns:\n",
    "        if final_df[col].dtype == 'object':\n",
    "            final_df[col] = final_df[col].fillna('unknown')\n",
    "        else:\n",
    "            final_df[col] = final_df[col].fillna(0)\n",
    "    \n",
    "    # Resetear índices\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Dataset personalizado creado con {len(final_df)} registros\")\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b854e5e0-ae8b-486a-9812-121d5b6aa7c9",
   "metadata": {},
   "source": [
    "#### process_conn_logs\n",
    "Función que procesa todos los archivos conn.log para crear el DataFrame base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c876390-d159-4d7e-83fe-6ca31a035de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conn_logs(normal_dir, attack_dir, columns_to_keep):\n",
    "    \n",
    "    normal_files = glob.glob(f\"{normal_dir}/**/conn.log\", recursive=True)\n",
    "    attack_files = glob.glob(f\"{attack_dir}/**/conn.log\", recursive=True)\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    # Procesar archivos de tráfico normal\n",
    "    for log_file in tqdm(normal_files, desc=\"Archivos normales\"):\n",
    "        df = read_zeek_log(log_file, columns_to_keep)  # Pasa las columnas a mantener\n",
    "        if df is not None and not df.empty:\n",
    "            df['isAttack'] = 0\n",
    "            df['typeAttack'] = 'normal'\n",
    "            dfs.append(df)\n",
    "    \n",
    "    # Procesar archivos de ataques\n",
    "    for log_file in tqdm(attack_files, desc=\"Archivos de ataque\"):\n",
    "        attack_type = determine_attack_type(log_file)\n",
    "        df = read_zeek_log(log_file, columns_to_keep)  # Pasa las columnas a mantener\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            df['isAttack'] = 1\n",
    "            df['typeAttack'] = attack_type\n",
    "            dfs.append(df)\n",
    "    \n",
    "    if dfs:\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540a665-a2a4-4960-b1bb-7f978a18abaf",
   "metadata": {},
   "source": [
    "#### process_log_type\n",
    "Función que procesa el resto de archivos.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a02d8d54-6d60-4a89-8e80-81e08e9d9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_type(log_type, normal_dir, attack_dir, columns):\n",
    "\n",
    "    normal_files = glob.glob(f\"{normal_dir}/**/{log_type}.log\", recursive=True)\n",
    "    attack_files = glob.glob(f\"{attack_dir}/**/{log_type}.log\", recursive=True)\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    # Procesar archivos\n",
    "    for log_file in normal_files + attack_files:\n",
    "        df = read_zeek_log(log_file, columns)\n",
    "        if df is not None and not df.empty:\n",
    "            dfs.append(df)\n",
    "    \n",
    "    if dfs:\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284de30-74eb-445a-ad1d-631794a7b13f",
   "metadata": {},
   "source": [
    "#### read_zeek_log\n",
    "Funcion que lee los archivos de LOG de Zeek y extrae las columnas especificadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cc6b5ff-c618-4aff-896f-1fd737ecf377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zeek_log(log_file, columns_to_keep=None):\n",
    "\n",
    "    try:\n",
    "        # Primero leemos los encabezados para obtener los nombres de las columnas\n",
    "        headers = None\n",
    "        separator = None\n",
    "        \n",
    "        with open(log_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('#fields'):\n",
    "                    headers = line.strip().split('\\t')[1:]\n",
    "                    break\n",
    "                if line.startswith('#separator'):\n",
    "                    separator_value = line.strip().split(' ')[1]\n",
    "                    if separator_value.startswith('\\\\x'):\n",
    "                        separator = bytes.fromhex(separator_value[2:]).decode('utf-8')\n",
    "                    else:\n",
    "                        separator = separator_value.encode().decode('unicode_escape')\n",
    "        \n",
    "        if headers is None or separator is None:\n",
    "            return None\n",
    "        \n",
    "        # Leer el archivo con pandas\n",
    "        df = pd.read_csv(log_file, \n",
    "                         comment='#', \n",
    "                         sep=separator, \n",
    "                         names=headers, \n",
    "                         na_values=['-'],\n",
    "                         quoting=3,\n",
    "                         error_bad_lines=False,\n",
    "                         warn_bad_lines=False,\n",
    "                         low_memory=False,\n",
    "                         encoding='utf-8')\n",
    "        \n",
    "        # Filtrar columnas si es necesario\n",
    "        if columns_to_keep is not None:\n",
    "            # Mantener solo columnas que existen en el DataFrame\n",
    "            cols_to_use = [col for col in columns_to_keep if col in df.columns]\n",
    "            df = df[cols_to_use]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer {log_file}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ab685f-1111-47bb-b264-236068ee5e5b",
   "metadata": {},
   "source": [
    "#### determine_attack_type\n",
    "Funcion que determina el tipo de ataque a partir de la ruta del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f95c94f-30eb-42fb-9511-7e2eb362b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_attack_type(file_path):\n",
    "\n",
    "    attack_mapping = {\n",
    "        \"Backdoor_attack\": \"backdoor\",\n",
    "        \"DDoS HTTP Flood\": \"ddos_http\",\n",
    "        \"DDoS ICMP Flood\": \"ddos_icmp\",\n",
    "        \"DDoS TCP SYN Flood\": \"ddos_tcp_syn\",\n",
    "        \"DDoS UDP Flood\": \"ddos_udp\",\n",
    "        \"MITM\": \"mitm_arp_dns\",\n",
    "        \"OS Fingerprinting\": \"os_fingerprint\",\n",
    "        \"Password\": \"password\",\n",
    "        \"Port Scanning\": \"port_scan\",\n",
    "        \"Ransomware\": \"ransomware\",\n",
    "        \"SQL injection\": \"sql_injection\",\n",
    "        \"Uploading\": \"upload\",\n",
    "        \"Vulnerability scanner\": \"vuln_scan\",\n",
    "        \"XSS\": \"xss\"\n",
    "    }\n",
    "    \n",
    "    for key, value in attack_mapping.items():\n",
    "        if key in file_path:\n",
    "            return value\n",
    "    \n",
    "    # Si no se encuentra correspondencia, extraer el nombre de la carpeta\n",
    "    parts = file_path.split(os.sep)\n",
    "    for part in parts:\n",
    "        for key, value in attack_mapping.items():\n",
    "            if key.lower() in part.lower():\n",
    "                return value\n",
    "    \n",
    "    return \"unknown_attack\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e51fcd-8a00-4801-b493-6a8d0ca5b695",
   "metadata": {},
   "source": [
    "## Ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888bb879-5eec-4a01-9967-a54f61197ab9",
   "metadata": {},
   "source": [
    "**Rutas de los directorios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccbfea04-f1bc-46e8-bc74-1d34fa49d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_logs_dir = \"../Zeek-Pipeline/zeek_logs/normal\"\n",
    "attack_logs_dir = \"../Zeek-Pipeline/zeek_logs/ataques\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542bc1ac-54bb-4f36-afb5-471bf3dcbf90",
   "metadata": {},
   "source": [
    "**Crear el dataset personalizado y guardarlo como \"NG-IIoTset.csv\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "915d5d91-e754-4f39-8b83-92e79ad83f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivos conn.log...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Archivos normales: 100%|██████████| 10/10 [00:01<00:00,  7.26it/s]\n",
      "Archivos de ataque: 100%|██████████| 14/14 [00:12<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base DataFrame creada con 7649572 registros\n",
      "Procesando archivos dns.log...\n",
      "  - Encontrados 24790 registros, fusionando...\n",
      "Procesando archivos mqtt_connect.log...\n",
      "  - Encontrados 666556 registros, fusionando...\n",
      "Procesando archivos mqtt_publish.log...\n",
      "  - Encontrados 666555 registros, fusionando...\n",
      "Procesando archivos modbus.log...\n",
      "  - Encontrados 98087 registros, fusionando...\n",
      "Procesando archivos http.log...\n",
      "  - Encontrados 196008 registros, fusionando...\n",
      "Procesando archivos files.log...\n",
      "  - Encontrados 282551 registros, fusionando...\n",
      "Procesando archivos weird.log...\n",
      "  - Encontrados 1176213 registros, fusionando...\n",
      "Aplicando muestreo estratificado personalizado...\n",
      "  - normal: 796782 registros originales, objetivo 10662000\n",
      "    Aumentando mediante duplicación a 10662000 registros\n",
      "  - backdoor: 1389 registros originales, objetivo 30000\n",
      "    Aumentando mediante duplicación a 30000 registros\n",
      "  - ddos_http: 43389 registros originales, objetivo 240000\n",
      "    Aumentando mediante duplicación a 240000 registros\n",
      "  - ddos_icmp: 2407982 registros originales, objetivo 2850000\n",
      "    Manteniendo los 2407982 registros originales\n",
      "  - ddos_tcp_syn: 1176730 registros originales, objetivo 2100000\n",
      "    Manteniendo los 1176730 registros originales\n",
      "  - ddos_udp: 3215182 registros originales, objetivo 3100000\n",
      "    Reduciendo mediante muestreo a 3100000 registros\n",
      "  - os_fingerprint: 303 registros originales, objetivo 5000\n",
      "    Aumentando mediante duplicación a 5000 registros\n",
      "  - mitm_arp_dns: 517 registros originales, objetivo 6000\n",
      "    Aumentando mediante duplicación a 6000 registros\n",
      "  - password: 96140 registros originales, objetivo 1000000\n",
      "    Aumentando mediante duplicación a 1000000 registros\n",
      "  - port_scan: 11362 registros originales, objetivo 45000\n",
      "    Aumentando mediante duplicación a 45000 registros\n",
      "  - ransomware: 1400 registros originales, objetivo 25000\n",
      "    Aumentando mediante duplicación a 25000 registros\n",
      "  - sql_injection: 4433 registros originales, objetivo 60000\n",
      "    Aumentando mediante duplicación a 60000 registros\n",
      "  - upload: 7636 registros originales, objetivo 50000\n",
      "    Aumentando mediante duplicación a 50000 registros\n",
      "  - vuln_scan: 19242108 registros originales, objetivo 150000\n",
      "    Reduciendo mediante muestreo a 150000 registros\n",
      "  - xss: 2105 registros originales, objetivo 25000\n",
      "    Aumentando mediante duplicación a 25000 registros\n",
      "Realizando limpieza final...\n",
      "Dataset personalizado creado con 18982712 registros\n"
     ]
    }
   ],
   "source": [
    "ng_iiotset_df = create_ng_iiotset()\n",
    "ng_iiotset_df.to_csv(\"NG-IIoTset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ae26f-9bee-4cff-9ac3-aae09c500614",
   "metadata": {},
   "source": [
    "**Comprobar todas las columnas del dataset final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79861414-184f-4dc3-a951-1ead3d46f367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Listado completo de características en el dataset :\n",
      "1. uid\n",
      "2. ts\n",
      "3. id.orig_h\n",
      "4. id.resp_h\n",
      "5. id.resp_p\n",
      "6. proto\n",
      "7. service\n",
      "8. conn_state\n",
      "9. duration\n",
      "10. orig_bytes\n",
      "11. resp_bytes\n",
      "12. orig_pkts\n",
      "13. resp_pkts\n",
      "14. ip_proto\n",
      "15. isAttack\n",
      "16. typeAttack\n",
      "17. query\n",
      "18. answers\n",
      "19. qtype_name\n",
      "20. rcode\n",
      "21. rcode_name\n",
      "22. connect_status\n",
      "23. client_id\n",
      "24. topic\n",
      "25. payload\n",
      "26. func\n",
      "27. pdu_type\n",
      "28. exception\n",
      "29. method\n",
      "30. uri\n",
      "31. user_agent\n",
      "32. host\n",
      "33. request_body_len\n",
      "34. response_body_len\n",
      "35. status_code\n",
      "36. fuid\n",
      "37. source\n",
      "38. mime_type\n",
      "39. filename\n",
      "40. seen_bytes\n",
      "41. total_bytes\n",
      "42. md5\n",
      "43. sha1\n",
      "44. sha256\n",
      "45. name\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nListado completo de características en el dataset :\")\n",
    "for i, column in enumerate(ng_iiotset_df.columns):\n",
    "    print(f\"{i+1}. {column}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
